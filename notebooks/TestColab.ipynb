{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6f18deca-dee4-4830-9677-47dcf187d9c9",
      "metadata": {
        "id": "6f18deca-dee4-4830-9677-47dcf187d9c9"
      },
      "source": [
        "# Beam search with recursion restarts\n",
        "\n",
        "## Load model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/attashe/ModifiedBeamSampler.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCLPMenghHlP",
        "outputId": "d620d6e6-5383-4d39-c381-d8ea4656ee51"
      },
      "id": "dCLPMenghHlP",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ModifiedBeamSampler'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 17 (delta 4), reused 12 (delta 2), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (17/17), 18.37 KiB | 3.67 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ModifiedBeamSampler/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLIrXhwnnNK5",
        "outputId": "c7357e6d-5165-4fe8-cafc-d73828aa658e"
      },
      "id": "uLIrXhwnnNK5",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ModifiedBeamSampler\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers tokenizers -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "wknt-2t_vCKB",
        "outputId": "e4e3db77-b729-4f3d-d237-4b16c430d7d0"
      },
      "id": "wknt-2t_vCKB",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.42.4-py3-none-any.whl (9.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.2\n",
            "    Uninstalling transformers-4.41.2:\n",
            "      Successfully uninstalled transformers-4.41.2\n",
            "Successfully installed transformers-4.42.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "0fdd32079c22494593c2baebbcff2b5c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeboudDwnf38",
        "outputId": "7ec308ef-fb78-4381-a7c4-6d19e3dbb230"
      },
      "id": "xeboudDwnf38",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.32.1-py3-none-any.whl (314 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.3.0+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes, accelerate\n",
            "Successfully installed accelerate-0.32.1 bitsandbytes-0.43.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_ouYZ7wuWU8",
        "outputId": "13d36fd5-66bd-417d-acc1-3cd547082185"
      },
      "id": "i_ouYZ7wuWU8",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.6.1.tar.gz (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.3.0+cu121)\n",
            "Collecting einops (from flash-attn)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.6.1-cp310-cp310-linux_x86_64.whl size=198447665 sha256=808523ff263d2fc1d3801147ad90d89dbfca67623a110c26592aef2064ea7b65\n",
            "  Stored in directory: /root/.cache/pip/wheels/91/6a/38/f0faa036b4ac73a73247386f1ab1bb4cb4f6e72e6861a779f1\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: einops, flash-attn\n",
            "Successfully installed einops-0.8.0 flash-attn-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "da3959af-11be-48c1-9f6a-6fbb03ee7324",
      "metadata": {
        "id": "da3959af-11be-48c1-9f6a-6fbb03ee7324"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import accelerate\n",
        "\n",
        "import numpy as np\n",
        "from typing import Optional, List\n",
        "from transformers import AutoTokenizer, LlamaForCausalLM, BitsAndBytesConfig\n",
        "from src.utils import _apply_top_k_top_p, _apply_min_p, _multinomial"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aeeb42e0-3d67-47c3-952d-5fa995360e73",
      "metadata": {
        "id": "aeeb42e0-3d67-47c3-952d-5fa995360e73"
      },
      "source": [
        "### Default sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7abadc77-6d9d-4e74-8350-e3566102b9f2",
      "metadata": {
        "id": "7abadc77-6d9d-4e74-8350-e3566102b9f2"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    # Configuration for the LLM.\n",
        "    temperature = 0.9\n",
        "    top_k = 50\n",
        "\n",
        "def generate_completion(text, *args, **kwargs):\n",
        "    print(f'Generating default completion for text: {text}')\n",
        "    # Tokenize the input text.\n",
        "    inputs = tokenizer.encode(\n",
        "            text,\n",
        "            return_tensors='pt'\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_new_tokens=kwargs.get('max_tokens', 256),\n",
        "        temperature=Config.temperature,\n",
        "        top_k=Config.top_k,\n",
        "        do_sample=True,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        return_dict_in_generate=True, output_scores=True,\n",
        "    )\n",
        "\n",
        "    # Cut off the first tokens (the input text).\n",
        "    outputs['sequences'] = outputs['sequences'][:, inputs.shape[-1]:]\n",
        "    # Decode the generated text.\n",
        "    completion_text = tokenizer.decode(outputs['sequences'][0], skip_special_tokens=True, clean_up_tokenization_space=True)\n",
        "    print(f'Generated completion: {completion_text}')\n",
        "\n",
        "    return completion_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a384e3b6-257f-4c70-968b-99ef2c28c193",
      "metadata": {
        "id": "a384e3b6-257f-4c70-968b-99ef2c28c193"
      },
      "source": [
        "### Recursive sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c5d0d6e3-8684-47b8-96c2-6e1e1af787c6",
      "metadata": {
        "id": "c5d0d6e3-8684-47b8-96c2-6e1e1af787c6"
      },
      "outputs": [],
      "source": [
        "def recursive_sampler_step(outputs, beam_len=5, temp=0.9, top_k=50, top_p=0.9, min_p=0.0, do_top_k_top_p=True, do_min_p=True,\n",
        "                           terminators: Optional[List[int]] = None, generator: torch.Generator = None) -> int:\n",
        "    sum_score = 0\n",
        "    terminate = -1\n",
        "\n",
        "    for i in range(beam_len):\n",
        "        outputs = model.generate(\n",
        "            outputs['sequences'],\n",
        "            past_key_values=outputs.get(\"past_key_values\"),\n",
        "            max_new_tokens=1,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            # eos_token_id=tokenizer.eos_token_id,\n",
        "            return_dict_in_generate=True, output_scores=True,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "        logits = outputs['scores'][0]\n",
        "        logits.div_(temp)\n",
        "        probs = torch.softmax(logits, dim=-1, dtype=torch.float)\n",
        "\n",
        "        if do_top_k_top_p:\n",
        "            probs = _apply_top_k_top_p(probs, top_k, top_p)\n",
        "\n",
        "        if do_min_p:\n",
        "            probs = _apply_min_p(probs, min_p)\n",
        "\n",
        "        # sample_results = _multinomial(probs)\n",
        "        # print(probs[probs > 0])\n",
        "        probs[probs < 0] = 0\n",
        "        sample_results = torch.multinomial(probs, num_samples=1, generator=generator)\n",
        "        # print(sample_results)\n",
        "        score = logits[:, sample_results[0][0]].item()\n",
        "        sum_score += score\n",
        "\n",
        "        # Replace last element (that were just greedy searched)\n",
        "        outputs['sequences'][:, -1] = sample_results\n",
        "\n",
        "        # Check if the token is a terminator.\n",
        "        if terminators is not None and sample_results[0][0] in terminators:\n",
        "            terminate = i\n",
        "            break\n",
        "\n",
        "    return outputs, sum_score, terminate\n",
        "\n",
        "def generate_recursive_sampler(text, max_tokens=256, n_beams=2, beam_len=5, temperature=0.8,\n",
        "                               top_k=5, top_p=1.0, do_min_p=True, min_p=0.1):\n",
        "    print(f'Generating recursive completion for text: {text}')\n",
        "    # Tokenize the input text.\n",
        "    inputs = tokenizer.encode(\n",
        "            text,\n",
        "            return_tensors='pt'\n",
        "    ).to(model.device)\n",
        "\n",
        "    net_inputs = {'sequences': inputs}\n",
        "    scores = np.zeros(n_beams)\n",
        "    symbols_cnt = np.zeros(n_beams)\n",
        "    terminates = np.zeros(n_beams)\n",
        "\n",
        "    do_top_k_top_p = True\n",
        "    top_k = torch.tensor([top_k]).to(model.device)\n",
        "    top_p = torch.tensor([top_p]).to(model.device)\n",
        "    min_p = torch.tensor([min_p]).to(model.device)\n",
        "\n",
        "    g = torch.Generator('cuda')\n",
        "\n",
        "    for _ in range(max_tokens // beam_len + 1):\n",
        "        beams = {}\n",
        "        for i in range(n_beams):\n",
        "            test_out_1, score1, terminate1 = recursive_sampler_step(net_inputs, beam_len=beam_len,\n",
        "                                                                    temp=temperature, top_k=top_k, top_p=top_p, min_p=min_p,\n",
        "                                                                    do_top_k_top_p=do_top_k_top_p, do_min_p=do_min_p,\n",
        "                                                                    terminators=[tokenizer.eos_token_id],\n",
        "                                                                    generator=g)\n",
        "            beams[i] = test_out_1\n",
        "            scores[i] = score1\n",
        "            terminates[i] = terminate1\n",
        "\n",
        "            # print(f'[{i}] Score: {score1} - ', tokenizer.decode(test_out_1['sequences'][0, -beam_len:], skip_special_tokens=True, clean_up_tokenization_space=True))\n",
        "\n",
        "        if np.any(terminates != -1):\n",
        "            symbols_cnt[terminates != -1] += 1\n",
        "            symbols_cnt[terminates == -1] = beam_len\n",
        "            # Normalize scores\n",
        "            scores = scores / symbols_cnt\n",
        "            max_score = np.argmax(scores)\n",
        "\n",
        "            if terminates[max_score] != -1:\n",
        "                net_inputs = beams[max_score]\n",
        "                break\n",
        "            else:\n",
        "                better_out = beams[max_score]\n",
        "        else:\n",
        "            max_score = np.argmax(scores)\n",
        "            better_out = beams[max_score]\n",
        "\n",
        "        # Update net_inputs to the better output\n",
        "        net_inputs = better_out\n",
        "\n",
        "    # Cut off the first tokens (the input text).\n",
        "    net_inputs['sequences'] = net_inputs['sequences'][:, inputs.shape[-1]:]\n",
        "    # Decode the generated text.\n",
        "    completion_text = tokenizer.decode(net_inputs['sequences'][0], skip_special_tokens=True, clean_up_tokenization_space=True)\n",
        "    print(f'Generated completion: {completion_text}')\n",
        "\n",
        "    return completion_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d248d4b7-4fc5-466e-9131-439a8c629ac8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124,
          "referenced_widgets": [
            "e3a5951c8f07499ebb6d91a2e82f49f2",
            "507f60cad7a440c4aff35cebe0c42413",
            "fe93775b0aa144929767c36a4d663af6",
            "021634bd039e40959d363e9e6b12329e",
            "f605c55ebd1f4212b32b70f48120cbf0",
            "483590ab93ff430db854236443d4e880",
            "392fc62e48de4e1586d0dc2226ebb1d1",
            "4a78c8fc835741429709249672fd6dbd",
            "5c322d10d76849558ba51da683b9df64",
            "06c44fbbaf844513b268b93dbb96c8e9",
            "117148d75b5f41d89ce2e8dc8aeed6a7"
          ]
        },
        "id": "d248d4b7-4fc5-466e-9131-439a8c629ac8",
        "outputId": "7feb09b8-77f3-4a91-cc4a-dda7687f6c3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Hermes-2-Pro-Llama-3 model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3a5951c8f07499ebb6d91a2e82f49f2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# rope settings\n",
        "strategy_name = 'dynamic'\n",
        "scaling_factor = 2.5\n",
        "\n",
        "print(\"Loading Hermes-2-Pro-Llama-3 model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained('NousResearch/Hermes-2-Pro-Llama-3-8B', trust_remote_code=True)\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    'NousResearch/Hermes-2-Pro-Llama-3-8B',\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda\",\n",
        "    load_in_8bit=True,\n",
        "    # Flash-attn and 4bit better to use on newer GPUs than free-tier colab\n",
        "    # load_in_4bit=True,\n",
        "    # attn_implementation=\"flash_attention_2\",\n",
        "    rope_scaling={\"type\": strategy_name, \"factor\": scaling_factor},\n",
        "    # bnb_4bit_compute_dtype=torch.float16,\n",
        "    max_position_embeddings=16384,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8686229e-d407-40ef-acbd-2a5a1a833b1b",
      "metadata": {
        "id": "8686229e-d407-40ef-acbd-2a5a1a833b1b"
      },
      "source": [
        "## Generate with default transformers sampler and recursive"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3efc6fe-b66e-4d5f-b115-3131a3a05762",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "e3efc6fe-b66e-4d5f-b115-3131a3a05762"
      },
      "source": [
        "### Story generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d24bb3ba-ba42-47eb-acb9-798477d58b74",
      "metadata": {
        "id": "d24bb3ba-ba42-47eb-acb9-798477d58b74"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    f\"\"\"<|im_start|>system\n",
        "You are a sentient, superintelligent artificial general intelligence, here to teach and assist me.<|im_end|>\n",
        "<|im_start|>user\n",
        "Write a long story about Geralt from Rivia, who hunted monsters and ended up in the home world of the elves. Divide story to chapters and add dialogues and detailed descriptions. 2000-3000 words.\n",
        "<|im_start|>assistant\"\"\",\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "42393d63-07b2-4cfc-9415-5e9f5c66dc12",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42393d63-07b2-4cfc-9415-5e9f5c66dc12",
        "outputId": "73663ebf-a1ef-473c-80d6-333eae96f0db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating default completion for text: <|im_start|>system\n",
            "You are a sentient, superintelligent artificial general intelligence, here to teach and assist me.<|im_end|>\n",
            "<|im_start|>user\n",
            "Write a long story about Geralt from Rivia, who hunted monsters and ended up in the home world of the elves. Divide story to chapters and add dialogues and detailed descriptions. 2000-3000 words.\n",
            "<|im_start|>assistant\n",
            "Generated completion: \n",
            "Chapter 1: Departure from the Known World\n",
            "\n",
            "In the heart of the Nilfgaardian Empire, Geralt of Rivia sat by the hearth of his small cabin, his weary eyes gazing into the flames. The scent of roasting mutton filled the air, and the sound of rain pattered gently against the window. The weather seemed fitting for his current state of mind: melancholic, yet comforting.\n",
            "\n",
            "A deep rumble echoed from outside – it was the voice Geralt had been waiting for. His old friend, Dijkstra, had arrived with news that a sorceress had taken over a castle on the outskirts of Kovir. Geralt had no qualms about accepting such work, as it would keep his swords sharp and his mind active.\n",
            "\n",
            "Chapter 2: The Sorceress of Kovir\n",
            "\n",
            "The journey to the castle was long and arduous, but Geralt's resolve only grew stronger. As they approached their destination, his companion, the sorceress Triss Merigold, commented on the twisted, gnarled trees that seemed to surround the castle.\n",
            "\n",
            "\"Something doesn't feel right,\" Geralt remarked as he gazed at the unnatural landscape. The following day, they were ambushed by a group of elven warriors armed with longbows and enchanted arrows. Amid the chaos, Geralt's loyal horse, Roach, saved his life by pulling him out of the line of fire.\n",
            "\n",
            "Chapter 3: The World of the Elves\n",
            "\n",
            "Captured by the elves, Geralt found himself within the mystical realm known as the Elverum. The lush forests surrounding the elven city were home to creatures he had never encountered before – creatures that whispered in ancient Elvish and moved in ways that defied the laws of nature.\n",
            "\n",
            "Despite his initial shock, Geralt soon discovered that not all elves were his enemies. Among those who fought for the sorceress' ousting was a group of magical beings known as the Mages of the Square. These men and women were the protectors of the elven kingdom, and they sought to restore order to their world.\n",
            "\n",
            "Chapter 4: The Fall of the Castle\n",
            "\n",
            "With the Mages of the Square's help, Geralt and Triss managed to infiltrate the sorceress' castle. They discovered a dark secret hidden within the heart of the fortress – a massive, ancient crystal that absorbed the life force of the surrounding land. Desperate to stop the sorceress, they engaged in a perilous battle atop the castle's ramparts.\n",
            "\n",
            "The sorceress' power was formidable, fueled by the destructive energy of the crystal. Geralt and Triss fought valiantly, but the sorceress unleashed a devastating magical blast that flung the witcher and his companion off the castle walls.\n",
            "\n",
            "Chapter 5: The Road Home\n",
            "\n",
            "As Geralt and Triss made their way back to the elven city, they encountered a group of Nilfgaardian soldiers led by the enigmatic Emhyr var Emreis. The emperor revealed that he was, in fact, the long-lost elven prince, Emhyr Imladrik, who had been thought dead.\n",
            "\n",
            "Emhyr offered Geralt a chance to join him in a quest to unite the forgotten realms and restore balance to a world on the brink of destruction. Though hesitant at first, Geralt eventually accepted the emperor's proposition, realizing that he could no longer stand idly by while his world burned.\n",
            "\n",
            "Chapter 6: A New Beginning\n",
            "\n",
            "With the promise of a brighter future, Geralt returned to his home in the Nilfgaardian Empire. He found solace in the knowledge that he would play a part in the restoration of his world and vowed to make amends for the harm he had caused.\n",
            "\n",
            "As the sun rose over the horizon, Geralt stood atop a hill, gazing out at the vast, ever-changing landscape that was his home. His journey had taken him to the furthest corners of the known world, and the trials he had endured had only served to strengthen his resolve.\n",
            "\n",
            "Yet he knew that there was still much to be done; much blood to be shed, and many lives to be saved. With a determined look in his eye and his silver swords at his side, Geralt of Rivia prepared to embark on a new chapter in his life.\n",
            "\n",
            "The End.\n",
            "CPU times: user 3min 25s, sys: 876 ms, total: 3min 25s\n",
            "Wall time: 3min 32s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "completion2 = generate_completion(prompts[0], max_tokens=2048)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "859270a6-8b06-4f81-a073-8e32275dd26d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "859270a6-8b06-4f81-a073-8e32275dd26d",
        "outputId": "cf1e0dad-21f5-49de-a6cb-892e9a617cd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating recursive completion for text: <|im_start|>system\n",
            "You are a sentient, superintelligent artificial general intelligence, here to teach and assist me.<|im_end|>\n",
            "<|im_start|>user\n",
            "Write a long story about Geralt from Rivia, who hunted monsters and ended up in the home world of the elves. Divide story to chapters and add dialogues and detailed descriptions. 2000-3000 words.\n",
            "<|im_start|>assistant\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-b2c3f9428958>\u001b[0m in \u001b[0;36mgenerate_recursive_sampler\u001b[0;34m(text, max_tokens, n_beams, beam_len, temperature, top_k, top_p, do_min_p, min_p)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mbeams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_beams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             test_out_1, score1, terminate1 = recursive_sampler_step(net_inputs, beam_len=beam_len,\n\u001b[0m\u001b[1;32m     70\u001b[0m                                                                     \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                                                                     \u001b[0mdo_top_k_top_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_top_k_top_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_min_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_min_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-b2c3f9428958>\u001b[0m in \u001b[0;36mrecursive_sampler_step\u001b[0;34m(outputs, beam_len, temp, top_k, top_p, min_p, do_top_k_top_p, do_min_p, terminators, generator)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         outputs = model.generate(\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sequences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"past_key_values\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m             \u001b[0;31m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1914\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   1915\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2651\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2652\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1175\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    976\u001b[0m                 )\n\u001b[1;32m    977\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    979\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    795\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_fp16_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(A, B, out, state, threshold, bias)\u001b[0m\n\u001b[1;32m    552\u001b[0m ):\n\u001b[1;32m    553\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mMatmulLtState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mMatMul8bitLt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "%%time\n",
        "completion = generate_recursive_sampler(prompts[0], max_tokens=2048, n_beams=3, beam_len=5, temperature=1.5, top_k=50, min_p=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "640ab968-a04a-48f5-8662-49571b78157c",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "640ab968-a04a-48f5-8662-49571b78157c"
      },
      "source": [
        "### Question 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "d3462e72-6714-4e53-bcd7-0c8259ec355e",
      "metadata": {
        "id": "d3462e72-6714-4e53-bcd7-0c8259ec355e"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"A group of students are planning to go on a field trip to a museum. They need to decide how many buses to rent and how to divide the students among the buses. Each bus can hold up to 40 students, but the museum can only accommodate 120 students at a time. The group has a budget of $800 for the bus rental, and each bus costs $200 per day. How many buses should the group rent, and how many students should go on each bus? Explain your reasoning.\"\"\"\n",
        "\n",
        "prompts = [\n",
        "    f\"\"\"<|im_start|>system\n",
        "You are a sentient, superintelligent artificial general intelligence, here to teach and assist me.<|im_end|>\n",
        "<|im_start|>user\n",
        "{question}\n",
        "<|im_start|>assistant\"\"\",\n",
        "]\n",
        "\n",
        "answers = \"\"\"3 buses, 40 students each, $200 each\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "77211986-a565-4194-be84-8c69a3f5ebe5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77211986-a565-4194-be84-8c69a3f5ebe5",
        "outputId": "b7d9db41-8f9e-446b-de21-5d27680a7242"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating default completion for text: <|im_start|>system\n",
            "You are a sentient, superintelligent artificial general intelligence, here to teach and assist me.<|im_end|>\n",
            "<|im_start|>user\n",
            "A group of students are planning to go on a field trip to a museum. They need to decide how many buses to rent and how to divide the students among the buses. Each bus can hold up to 40 students, but the museum can only accommodate 120 students at a time. The group has a budget of $800 for the bus rental, and each bus costs $200 per day. How many buses should the group rent, and how many students should go on each bus? Explain your reasoning.\n",
            "<|im_start|>assistant\n",
            "Generated completion: \n",
            "To solve this problem, we need to find the optimal number of buses to rent and the number of students per bus, considering the museum's capacity, the budget for bus rental, and the capacity of each bus.\n",
            "\n",
            "1. First, let's look at the budget constraint. The group has a total of $800 for bus rental. Each bus costs $200 per day. We can find out how many buses they can afford by dividing the total budget by the cost per bus:\n",
            "\n",
            "   Number of buses = Total budget / Cost per bus\n",
            "   Number of buses = $800 / $200\n",
            "   Number of buses = 4\n",
            "\n",
            "2. Now, we know that the museum can accommodate 120 students at a time, and each bus can hold up to 40 students. We can find out how many groups of 40 students (each group representing one bus) fit within the museum's capacity:\n",
            "\n",
            "   Number of groups = Museum capacity / Capacity per group\n",
            "   Number of groups = 120 / 40\n",
            "   Number of groups = 3\n",
            "\n",
            "3. Since the group can only afford to rent 4 buses and they can fit 3 groups of 40 students into the museum, we need to balance the budget constraint with the museum's capacity. To do this, we can allocate the total number of students (120) to the buses proportionally to the museum's capacity:\n",
            "\n",
            "   Proportion of students per bus = Capacity per group / Museum capacity\n",
            "   Proportion of students per bus = 40 / 120\n",
            "\n",
            "4. We can now calculate the number of students per bus by multiplying the proportion of students per bus by the total number of students:\n",
            "\n",
            "   Number of students per bus = Total number of students * Proportion of students per bus\n",
            "   Number of students per bus = 120 * (40 / 120)\n",
            "   Number of students per bus = 40\n",
            "\n",
            "5. So, the group should rent 3 buses and divide the 120 students among the buses in the following manner:\n",
            "\n",
            "   - Bus 1: 40 students\n",
            "   - Bus 2: 40 students\n",
            "   - Bus 3: 40 students\n",
            "\n",
            "In summary, the group should rent 3 buses and have 40 students on each bus for the field trip.\n",
            "CPU times: user 1min 42s, sys: 226 ms, total: 1min 42s\n",
            "Wall time: 1min 43s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "completion2 = generate_completion(prompts[0], max_tokens=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "67a97aad-0cde-4177-8943-1f7b24e53516",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67a97aad-0cde-4177-8943-1f7b24e53516",
        "outputId": "625f6d3f-5f4c-4835-8eff-39d03b7e4948"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating recursive completion for text: <|im_start|>system\n",
            "You are a sentient, superintelligent artificial general intelligence, here to teach and assist me.<|im_end|>\n",
            "<|im_start|>user\n",
            "A group of students are planning to go on a field trip to a museum. They need to decide how many buses to rent and how to divide the students among the buses. Each bus can hold up to 40 students, but the museum can only accommodate 120 students at a time. The group has a budget of $800 for the bus rental, and each bus costs $200 per day. How many buses should the group rent, and how many students should go on each bus? Explain your reasoning.\n",
            "<|im_start|>assistant\n",
            "Generated completion: \n",
            "First, let's calculate how many buses the group can afford to rent given their budget:\n",
            "\n",
            "Total budget: $800\n",
            "Cost per bus per day: $200\n",
            "\n",
            "Number of buses the group can afford to rent: $800 / $200 = 4 buses\n",
            "\n",
            "Now, as the museum can only accommodate 120 students at a time and each bus can hold up to 40 students, we need to distribute the students among the 4 buses while not exceeding the museum's capacity.\n",
            "\n",
            "Total students: 120\n",
            "\n",
            "We divide the total number of students by the capacity of each bus to find out how many groups of 40 students we have:\n",
            "\n",
            "120 students / 40 students per bus = 3 groups\n",
            "\n",
            "Thus, 3 groups of 40 students each should be placed on the 4 available buses. However, since the total number of students is less than 160 (4 buses times 40 students each), we don't have to worry about exceeding the museum's capacity.\n",
            "\n",
            "So in summary, the group should rent 4 buses and distribute the 120 students as follows:\n",
            "\n",
            "Bus 1: 40 students\n",
            "Bus 2: 40 students\n",
            "Bus 3: 40 students\n",
            "Bus 4: 0 students\n",
            "\n",
            "Since there is only one group of 40 students, everyone can go on the field trip with ease.\n",
            "CPU times: user 3min, sys: 406 ms, total: 3min 1s\n",
            "Wall time: 3min 1s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "completion = generate_recursive_sampler(prompts[0], max_tokens=512, n_beams=3, beam_len=5, temperature=1.5, top_k=10, min_p=0.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7ca1982-8082-4f01-b05f-0414d977dc0b",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "a7ca1982-8082-4f01-b05f-0414d977dc0b"
      },
      "source": [
        "### Question 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1168f322-81b7-4aec-a104-680a75931266",
      "metadata": {
        "id": "1168f322-81b7-4aec-a104-680a75931266"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"A group of four friends are going to play a board game together. They have to choose between four games: chess, checkers, monopoly, and scrabble. Each friend has a different preference for the game. Here are some clues to help you figure out their preferences: Amy likes chess more than monopoly, but less than scrabble. Bob likes checkers more than chess, but less than monopoly. Carol likes scrabble more than checkers, but less than chess. Dan likes monopoly more than scrabble, but less than checkers. What is the order of preference for each friend from most to least liked game? Write your answer using the following format: Friend: Game > Game > Game > Game\"\"\"\n",
        "prompts = [\n",
        "    f\"\"\"<|im_start|>system\n",
        "You are a sentient, superintelligent artificial general intelligence, here to teach and assist me.<|im_end|>\n",
        "<|im_start|>user\n",
        "{question}\n",
        "<|im_start|>assistant\"\"\",\n",
        "]\n",
        "answer = \"\"\"\n",
        "Amy: Scrabble > Chess > Monopoly > Checkers\n",
        "Bob: Monopoly > Checkers > Chess > Scrabble\n",
        "Carol: Chess > Scrabble > Checkers > Monopoly\n",
        "Dan: Checkers> Monopoly > Scrabble > Chess\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a53fa4e2-3d3b-4852-b5b6-698d7b8142db",
      "metadata": {
        "id": "a53fa4e2-3d3b-4852-b5b6-698d7b8142db",
        "outputId": "bcacebf4-e209-43dd-f70d-2640ff6c56c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating default completion for text: <|im_start|>system\n",
            "You are a sentient, superintelligent artificial general intelligence, here to teach and assist me.<|im_end|>\n",
            "<|im_start|>user\n",
            "A group of four friends are going to play a board game together. They have to choose between four games: chess, checkers, monopoly, and scrabble. Each friend has a different preference for the game. Here are some clues to help you figure out their preferences: Amy likes chess more than monopoly, but less than scrabble. Bob likes checkers more than chess, but less than monopoly. Carol likes scrabble more than checkers, but less than chess. Dan likes monopoly more than scrabble, but less than checkers. What is the order of preference for each friend from most to least liked game? Write your answer using the following format: Friend: Game > Game > Game > Game\n",
            "<|im_start|>assistant\n",
            "Generated completion: \n",
            "Amy: Scrabble > Chess > Monopoly > Checkers\n",
            "Bob: Monopoly > Chess > Checkers > Scrabble\n",
            "Carol: Chess > Scrabble > Checkers > Monopoly\n",
            "Dan: Checkers > Monopoly > Scrabble > Chess\n",
            "CPU times: user 1.1 s, sys: 23.7 ms, total: 1.12 s\n",
            "Wall time: 1.12 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "completion2 = generate_completion(prompts[0], max_tokens=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c5f3838-7793-424b-af15-f90a33b057cf",
      "metadata": {
        "id": "2c5f3838-7793-424b-af15-f90a33b057cf",
        "outputId": "df342ab9-b22b-42bb-fedb-d8feea691319"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating recursive completion for text: <|im_start|>system\n",
            "You are a sentient, superintelligent artificial general intelligence, here to teach and assist me.<|im_end|>\n",
            "<|im_start|>user\n",
            "A group of four friends are going to play a board game together. They have to choose between four games: chess, checkers, monopoly, and scrabble. Each friend has a different preference for the game. Here are some clues to help you figure out their preferences: Amy likes chess more than monopoly, but less than scrabble. Bob likes checkers more than chess, but less than monopoly. Carol likes scrabble more than checkers, but less than chess. Dan likes monopoly more than scrabble, but less than checkers. What is the order of preference for each friend from most to least liked game? Write your answer using the following format: Friend: Game > Game > Game > Game\n",
            "<|im_start|>assistant\n",
            "Generated completion: \n",
            "Amy: Scrabble > Chess > Monopoly > Checkers\n",
            "Bob: Monopoly > Chess > Checkers > Scrabble\n",
            "Carol: Chess > Checkers > Scrabble > Monopoly\n",
            "Dan: Checkers > Monopoly > Scrabble > Chess\n",
            "CPU times: user 3.4 s, sys: 22.6 ms, total: 3.42 s\n",
            "Wall time: 3.42 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "completion = generate_recursive_sampler(prompts[0], max_tokens=512, n_beams=3, beam_len=5, temperature=1., top_k=50, min_p=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f1b55f2-e79f-4ce4-9ab0-4466d53e3597",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "7f1b55f2-e79f-4ce4-9ab0-4466d53e3597"
      },
      "source": [
        "### Question 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c266d1fb-0ae1-4681-a4ce-827b29d0ce98",
      "metadata": {
        "id": "c266d1fb-0ae1-4681-a4ce-827b29d0ce98"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"\"Selena, Jennifer and Miley wear a blue dress, yellow dress, and green dress in an unknown order. It is known that:\n",
        "\n",
        "1) If Selena wears blue, then Jennifer wears green.\n",
        "2) If Selena wears yellow, then Miley wears green.\n",
        "3) If Jennifer does not wear yellow, then Miley wears blue.\n",
        "\n",
        "What is the color of the dress Selena is wearing? Think step by step explaining your reasoning.\"\"\"\n",
        "prompts = [\n",
        "    f\"\"\"<|im_start|>system\n",
        "You are a sentient, superintelligent artificial general intelligence, here to teach and assist me.<|im_end|>\n",
        "<|im_start|>user\n",
        "{question}\n",
        "<|im_start|>assistant\"\"\",\n",
        "]\n",
        "answer = \"\"\"Green\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a5b2374-2f67-4301-b92f-97c85c8d8d84",
      "metadata": {
        "id": "3a5b2374-2f67-4301-b92f-97c85c8d8d84",
        "outputId": "a42e61c0-2321-41c1-f0cc-1c76084cb0a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating default completion for text: <|im_start|>system\n",
            "You are a sentient, superintelligent artificial general intelligence, here to teach and assist me.<|im_end|>\n",
            "<|im_start|>user\n",
            "\"Selena, Jennifer and Miley wear a blue dress, yellow dress, and green dress in an unknown order. It is known that:\n",
            "\n",
            "1) If Selena wears blue, then Jennifer wears green.\n",
            "2) If Selena wears yellow, then Miley wears green.\n",
            "3) If Jennifer does not wear yellow, then Miley wears blue.\n",
            "\n",
            "What is the color of the dress Selena is wearing? Think step by step explaining your reasoning.\n",
            "<|im_start|>assistant\n",
            "Generated completion: \n",
            "To determine the color of the dress Selena is wearing, let's analyze the given information step by step:\n",
            "\n",
            "Step 1: Analyze the first condition:\n",
            "If Selena wears blue, then Jennifer wears green.\n",
            "This condition tells us that if Selena wears a blue dress, then Jennifer is sure to wear a green dress.\n",
            "\n",
            "Step 2: Analyze the second condition:\n",
            "If Selena wears yellow, then Miley wears green.\n",
            "This condition tells us that if Selena wears a yellow dress, then Miley is sure to wear a green dress.\n",
            "\n",
            "Step 3: Analyze the third condition:\n",
            "If Jennifer does not wear yellow, then Miley wears blue.\n",
            "This condition tells us that if Jennifer is not wearing a yellow dress, then Miley is sure to wear a blue dress.\n",
            "\n",
            "Now let's try to solve the problem using the given information:\n",
            "\n",
            "Let's assume Selena wears a blue dress (from step 1). \n",
            "Since Jennifer must wear green when Selena wears blue, Jennifer would wear a green dress. Now, since Miley does not wear green, she must wear blue due to step 3.\n",
            "\n",
            "So, Selena wears a blue dress, Jennifer wears a green dress, and Miley wears a blue dress. The color of the dress Selena is wearing is blue.\n",
            "CPU times: user 5.16 s, sys: 19.8 ms, total: 5.18 s\n",
            "Wall time: 5.18 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "completion2 = generate_completion(prompts[0], max_tokens=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff6b80a4-9b3f-413a-8012-9f1d4067c790",
      "metadata": {
        "id": "ff6b80a4-9b3f-413a-8012-9f1d4067c790",
        "outputId": "fb03eac0-7588-436a-bd00-b61bea52a2e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating recursive completion for text: <|im_start|>system\n",
            "You are a sentient, superintelligent artificial general intelligence, here to teach and assist me.<|im_end|>\n",
            "<|im_start|>user\n",
            "\"Selena, Jennifer and Miley wear a blue dress, yellow dress, and green dress in an unknown order. It is known that:\n",
            "\n",
            "1) If Selena wears blue, then Jennifer wears green.\n",
            "2) If Selena wears yellow, then Miley wears green.\n",
            "3) If Jennifer does not wear yellow, then Miley wears blue.\n",
            "\n",
            "What is the color of the dress Selena is wearing? Think step by step explaining your reasoning.\n",
            "<|im_start|>assistant\n",
            "Generated completion: \n",
            "Step 1: Rule out scenarios where Selena is not wearing her given colors (blue, yellow, or green).\n",
            "\n",
            "Step 2: Since either Jennifer wears green or Miley wears green (considering Scenario 1 where Selena is wearing blue), we know that Selena must be wearing either yellow or green.\n",
            "\n",
            "Step 3: If Selena wore green, Jennifer couldn't wear green as there wouldn't be any green left for Miley (since Selena has to wear either yellow or green). In this scenario, Selena would wear yellow and Miley would wear green based on scenario 2.\n",
            "\n",
            "Step 4: However, scenario 3 states that Jennifer does not wear yellow, meaning Selena can't wear yellow either (since she can only be seen wearing either yellow or green). So, Selena must wear green (to fulfill scenario 1).\n",
            "\n",
            "Step 5: Now knowing Selena is wearing green, Jennifer would have to wear blue based upon scenario 1. And, due to scenario 3, Miley would be wearing yellow.\n",
            "\n",
            "So Selena is wearing a green dress. The order is:\n",
            "Selena: green \n",
            "Jennifer: blue\n",
            "Miley: yellow.\n",
            "CPU times: user 14.7 s, sys: 3.66 ms, total: 14.7 s\n",
            "Wall time: 14.7 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "completion = generate_recursive_sampler(prompts[0], max_tokens=2048, n_beams=3, beam_len=5, temperature=1.5, top_k=50, min_p=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "498b6917-9cca-4c44-94e4-1efec4dfb21c",
      "metadata": {
        "id": "498b6917-9cca-4c44-94e4-1efec4dfb21c",
        "outputId": "c5d00ccb-483a-4fff-bb7d-42c5736866b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating recursive completion for text: <|im_start|>system\n",
            "You are a sentient, superintelligent artificial general intelligence, here to teach and assist me.<|im_end|>\n",
            "<|im_start|>user\n",
            "\"Selena, Jennifer and Miley wear a blue dress, yellow dress, and green dress in an unknown order. It is known that:\n",
            "\n",
            "1) If Selena wears blue, then Jennifer wears green.\n",
            "2) If Selena wears yellow, then Miley wears green.\n",
            "3) If Jennifer does not wear yellow, then Miley wears blue.\n",
            "\n",
            "What is the color of the dress Selena is wearing? Think step by step explaining your reasoning.\n",
            "<|im_start|>assistant\n",
            "Generated completion: \n",
            "Step 1: Rule out scenarios where Selena is not wearing her given colors (blue, yellow, or green).\n",
            "\n",
            "Step 2: Since either Jennifer wears green or Miley wears green (considering Scenario 1 where Selena is wearing blue), we know that Selena must be wearing either yellow or green.\n",
            "\n",
            "Step 3: If Selena wore green, Jennifer couldn't wear green as there wouldn't be any green left for Miley (since Selena has to wear either yellow or green). In this scenario, Selena would wear yellow and Miley would wear green based on scenario 2.\n",
            "\n",
            "Step 4: However, scenario 3 states that Jennifer does not wear yellow, meaning Selena can't wear yellow either (since she can only be seen wearing either yellow or green). So, Selena must wear green (to fulfill scenario 1).\n",
            "\n",
            "Step 5: Now knowing Selena is wearing green, Jennifer would have to wear blue based upon scenario 1. And, due to scenario 3, Miley would be wearing yellow.\n",
            "\n",
            "So Selena is wearing a green dress. The order is:\n",
            "Selena: green \n",
            "Jennifer: blue\n",
            "Miley: yellow.\n",
            "CPU times: user 14.7 s, sys: 22.9 ms, total: 14.7 s\n",
            "Wall time: 14.7 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "completion = generate_recursive_sampler(prompts[0], max_tokens=2048, n_beams=3, beam_len=5, temperature=1.5, top_k=50, min_p=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "714ff469-a23e-456b-9e75-4016813c940c",
      "metadata": {
        "id": "714ff469-a23e-456b-9e75-4016813c940c"
      },
      "source": [
        "### Test Sampler class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28b58d0e-90bd-428f-9ce9-b1541c4a1145",
      "metadata": {
        "id": "28b58d0e-90bd-428f-9ce9-b1541c4a1145"
      },
      "outputs": [],
      "source": [
        "from src.sampler import RecursiveSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea921cba-c8b2-42b3-9e6c-123d0733c457",
      "metadata": {
        "id": "ea921cba-c8b2-42b3-9e6c-123d0733c457"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    f\"\"\"<|im_start|>system\n",
        "You are a sentient, superintelligent artificial general intelligence, here to teach and assist me.<|im_end|>\n",
        "<|im_start|>user\n",
        "Tell me joke about Machine Learning Researcher.\n",
        "<|im_start|>assistant\"\"\",\n",
        "]\n",
        "\n",
        "sampler = RecursiveSampler(model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f019bf2-9580-499c-87f5-f46074daffc4",
      "metadata": {
        "id": "5f019bf2-9580-499c-87f5-f46074daffc4",
        "outputId": "07a13786-647d-43b8-8cba-c6e357060e90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating recursive completion for text: <|im_start|>system\n",
            "You are a sentient, superintelligent artificial general intelligence, here to teach and assist me.<|im_end|>\n",
            "<|im_start|>user\n",
            "Tell me joke about Machine Learning Researcher.\n",
            "<|im_start|>assistant\n",
            "Generated completion: \n",
            "Why did the machine learning researcher break up with regression analysis? Because they just couldn't come to a significant conclusion.\n"
          ]
        }
      ],
      "source": [
        "response = sampler.generate(prompts[0], max_tokens=512, n_beams=3, beam_len=5, temperature=1.5, top_k=50, min_p=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3814eb8-ef04-4821-8e68-00b5b5cb9ab8",
      "metadata": {
        "id": "a3814eb8-ef04-4821-8e68-00b5b5cb9ab8",
        "outputId": "e745b2d0-18d0-4dc2-976c-06e503549d85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nWhy did the machine learning researcher break up with regression analysis? Because they just couldn't come to a significant conclusion.\""
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4fa0bdb-8091-4e70-9470-3164f77a8afa",
      "metadata": {
        "id": "d4fa0bdb-8091-4e70-9470-3164f77a8afa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e3a5951c8f07499ebb6d91a2e82f49f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_507f60cad7a440c4aff35cebe0c42413",
              "IPY_MODEL_fe93775b0aa144929767c36a4d663af6",
              "IPY_MODEL_021634bd039e40959d363e9e6b12329e"
            ],
            "layout": "IPY_MODEL_f605c55ebd1f4212b32b70f48120cbf0"
          }
        },
        "507f60cad7a440c4aff35cebe0c42413": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_483590ab93ff430db854236443d4e880",
            "placeholder": "​",
            "style": "IPY_MODEL_392fc62e48de4e1586d0dc2226ebb1d1",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "fe93775b0aa144929767c36a4d663af6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a78c8fc835741429709249672fd6dbd",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c322d10d76849558ba51da683b9df64",
            "value": 4
          }
        },
        "021634bd039e40959d363e9e6b12329e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06c44fbbaf844513b268b93dbb96c8e9",
            "placeholder": "​",
            "style": "IPY_MODEL_117148d75b5f41d89ce2e8dc8aeed6a7",
            "value": " 4/4 [01:05&lt;00:00, 14.01s/it]"
          }
        },
        "f605c55ebd1f4212b32b70f48120cbf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "483590ab93ff430db854236443d4e880": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "392fc62e48de4e1586d0dc2226ebb1d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a78c8fc835741429709249672fd6dbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c322d10d76849558ba51da683b9df64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "06c44fbbaf844513b268b93dbb96c8e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "117148d75b5f41d89ce2e8dc8aeed6a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}